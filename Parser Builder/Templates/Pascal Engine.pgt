##TEMPLATE-NAME 'Pascal - inline engine V1.0 - Rob F.M. van den Brink'
##LANGUAGE 'Object Pascal (Delphi, Virtual Pascal, etc)'
##ENGINE-NAME 'Pascal inline engine, by Rob F.M. van den Brink'
##AUTHOR 'Rob F.M. van den Brink - the Netherlands' 
##FILE-EXTENSION 'pas'
##NOTES
This template generates an inline parser engine, in (object) pascal code,
with full support of error productions (to recover from errors).
The lexer and parser are fully embedded in the rest of your code, so that 
other modules are not required.

The resulting code can be compiled directly into a stand-alone program with 
a good object pascal compiler (like Virtual Pascal or Delphi). The
resulting progam will read an input file (specified on the command line)
and emits the tokens and associated actions to the screen.
Since this engine has full support of error productions, and uses the SynError
token for that, you should define 'SynError' in the "virtual terminals" of your 
grammar.

Version V1.0, June 2006
(c) Rob F.M. van den Brink - the Netherlands, 2006 - R.F.M.vandenBrink@hccnet.nl

Useful compilers are: Virtual Pascal (www.vpascal.com - freeware, recommended), Delphi (www.borland.com - payware, recommended), Free Pascal (www.freepascal.org - freeware), Kylix (not tested), GNU pascal (not tested).

Please download the complete engine on the Engine Download page. It contains several examples.
##END-NOTES
##PARAMETERS
Program %OutputFileBase%;
uses Objects;
##END-PARAMETERS
##ID-SEPARATOR '_'
##ID-CASE Uppercase
##ID-SYMBOL-PREFIX 'Tag'
##DELIMITER ','
//---<This file is auto generated by GoldParser>-------------------------------
// Pascal_Engine.pgt, Version 1.0, June 2006
// (c) Rob F.M. van den Brink, the Netherlands, R.F.M.vandenBrink@hccnet.nl
//-----------------------------------------------------------------------------
  {$UNDEF  Debug_LEXER    //define if dump of lexer  (DFA)  states is required} 
  {$DEFINE Debug_TOKENS   //define if dump of tokenized lexems is required} 
  {$UNDEF  Debug_PARSER   //define if dump of parser (LALR) states is required} 
  {$DEFINE Debug_ACTIONS  //define if dump of called actions is required} 
  {$DEFINE Debug_ERRORS   //undefine if not used} 
  {$UNDEF  Debug_HEAPMEM  //define if dump of memory heap is required} 
//-----------------------------------------------------------------------------

const Tab=chr(9); LF=Chr(10); CR=Chr(13);  FF=Chr(12); EOF=Chr(26);

//-----------------------------------------------------------------------------
// ERROR TAGS
//-----------------------------------------------------------------------------
const Err_Lexeme        = 1;
      Err_Skipped       = 2; 
      Err_Halted	= 3;
      Err_Overflow      = 4;
      Err_StackOverflow = 5;

//-----------------------------------------------------------------------------
// TOKEN TAGS
//-----------------------------------------------------------------------------
##SYMBOL-TABLE
const TagCount = %Count%;
##SYMBOLS
      %ID.Padded%  = %Value.Padded%;  // Kind=%Kind% %Description%
##END-SYMBOLS
##END-SYMBOL-TABLE
type  TTokenTag    = 0..TagCount;
      TToken       = record 
                       Tag:TTokenTag; 
                       LinePos, CharPos: Integer; 
                       Kind:0..5; 
                     end;
      PToken       = ^TToken;
      TTokenList   = array [1..100] of TToken;
      PTokenList   = ^TTokenList;
      PText        = ^Text;
var   Fo,Fe:PText;

//-----------------------------------------------------------------------------
// OBJECTS (SCANNER,LEXER,PARSER)
//-----------------------------------------------------------------------------
Type
  TScanner = object(TObject)
    Fi:Text;
    CurChar:    Char;
    NextChar:   Char;
    procedure   Get; virtual;
    constructor Init(FileName:String); 
    destructor  Done; Virtual;
  end;  

  
  TLexer=object(TScanner)
    ErrorMax:   Integer;
    ErrorCnt:   Integer;
    LinePos:	Integer;
    CharPos:	Integer;
    LexemeLen:  Integer;
    LexemeStr:  String;
    Token:      TToken;
    constructor Init(FileName:String); 
    procedure   GetChar; 
    procedure   GetLine; 
    procedure   GetToken; virtual;
    function    GetLexeme(SkipMode:Boolean):Integer; 
    procedure   DoError  (ErrorNr:Integer; TokenPtr: PToken);
  end;

  
  PParser = ^TParser;
  TParser = object(TLexer)
    constructor Init(InFile:String);
    destructor  Done; virtual;
    Procedure   DoAction (T:PTokenList; RuleNr,TokenCnt:Integer);
    procedure   DoAccept (TokenPtr: PToken);
    Procedure   DoCleanup(TokenPtr: PToken);
    function    DoParse  (StackSize:Integer):Integer;
##PARAMETERS    
    // the rules below are auto-generated from the grammar: '%NAME%' 
##END-PARAMETERS
##RULES
    procedure   Action_%ID.Padded%(T:PTokenList);
##END-RULES
  end {TParser};



{$IFDEF Debug_HEAPMEM} 
{-----------------------------------------------------------------------------}
procedure ShowMem(Prefix:String);
{-----------------------------------------------------------------------------}
var Status:theapstatus;
begin
  Status:=GetHeapStatus;
  Writeln(Fo^, Prefix,': ',Status.TotalAllocated:1, ' bytes'); // [',HeapCount:1,':',HeapSize,']');
end;
{$ENDIF}


{-----------------------------------------------------------------------------}
constructor TScanner.Init(FileName:String);
{-----------------------------------------------------------------------------}
begin
  inherited init;
  assign(Fi,FileName);
  reset(Fi);
  CurChar:=#0;
  NextChar:=#0;
  Get;
end;


{-----------------------------------------------------------------------------}
destructor TScanner.Done;
{-----------------------------------------------------------------------------}
begin
  close(Fi);
  inherited done;
end;


{-----------------------------------------------------------------------------}
Procedure TScanner.Get; //replace this one with your own reader
{-----------------------------------------------------------------------------}
begin
  CurChar:=NextChar;
  if NextChar<>EOF then Read(Fi,NextChar); 
end;


{-----------------------------------------------------------------------------}
constructor TLexer.Init(FileName:String);
{-----------------------------------------------------------------------------}
begin
  inherited init(FileName);
  LexemeLen:=0;
  LexemeStr:='';
  LinePos:=1;
  CharPos:=0;
  ErrorMax:=100;
  ErrorCnt:=0;
  Token.Tag:=0;
  Token.LinePos:=LinePos;
  Token.CharPos:=CharPos;
  GetChar;
end;


{-----------------------------------------------------------------------------}
Procedure TLexer.GetChar; 
{-----------------------------------------------------------------------------}
begin  
  case CurChar of
    LF:    begin CharPos:=0; inc(LinePos);      end;
    CR,FF: begin CharPos:=0;                    end;
    TAB:   begin CharPos:= (CharPos div 8)*8+8; end;
  end; {case} 
  inc(LexemeLen); inc(CharPos);
  if LexemeLen<256 then begin
    setlength(LexemeStr,LexemeLen);
    LexemeStr[LexemeLen]:=CurChar;
  end {else overflow};
  Get;
end;


{-----------------------------------------------------------------------------}
Procedure TLexer.GetLine;
{-----------------------------------------------------------------------------}
begin
  repeat
    case CurChar of
      CR,FF,LF: Break;
      else      Get;
    end; {case}
  until false;
  repeat
    case CurChar of
      LF:    begin CharPos:=0; inc(LinePos);      end;
      CR,FF: begin CharPos:=0;                    end;
      else   Break;
    end;
    Get;
  until false;
  inc(CharPos);
  LexemeStr:='';
  LexemeLen:=0;
end;



{-----------------------------------------------------------------------------}
procedure TLexer.GetToken;
{-----------------------------------------------------------------------------}
// Reads a "token", being a representation of a lexeme.
// this means, reading a lexeme string and bind it to a token
// Extend or change this function according to your application needs
//
// A token is a record, usually containing information like
//   - a tag being unique for each lexeme, 
//   - a pointer to a string to that lexeme, 
//   - the location of that lexeme
//   - and everything that is needed for your application
{-----------------------------------------------------------------------------}
var CommentLevel:Integer;
    Tag:Integer;
begin
  CommentLevel:=0;
  repeat
    if CommentLevel=0 then begin
      Tag:=GetLexeme(False);
      case Tag of
        TAG_WHITESPACE:    {$IFDEF Debug_LEXER} WriteLn(Fo^,'  ---WhiteSpace---') {$ENDIF};
        TAG_COMMENTLINE:   GetLine;
        TAG_COMMENTSTART:  Inc(CommentLevel);
        TAG_COMMENTEND,-1: begin inc(ErrorCnt); DoError(Err_Lexeme, Nil); GetChar;   end;
        else               break;
      end;
    end else begin
      Tag:=GetLexeme(True);
      case Tag of
        TAG_COMMENTSTART:  Inc(CommentLevel);
        TAG_COMMENTEND:    Dec(CommentLevel);
        TAG_EOF:           break;
      end;
    end;
  until False;
  Token.Tag :=Tag;
  Token.Kind:=1; //flag this token as an "terminal" token
  {$IFDEF Debug_TOKENS} writeln(Fo^,'Token(',Token.LinePos:1,':',Token.CharPos:1,').Tag=',Tab,Token.Tag,Tab,'"',LexemeStr,'"'); {$ENDIF}
end;



{-----------------------------------------------------------------------------}
Procedure TLexer.DoError(ErrorNr:Integer; TokenPtr:PToken);
{-----------------------------------------------------------------------------}
//Augment this with you own code, to handle errors
begin
  case ErrorNr of
    Err_StackOverflow:  
         begin
           writeln(Fe^,'Syntax Error: Overflow of parse stack at');
           writeln(Fe^,'  Pos(',Token.LinePos:1,':',Token.CharPos:1,')=',Tab,Token.Tag,Tab,'"',LexemeStr,'"');
         end;
    Err_Overflow:  
         begin
           writeln(Fe^,'--> too many errors (',ErrorMax,'), parsing stopped');
         end;
    Err_Skipped:
         begin
           dec(TokenPtr);
           write(Fe^,'--> Invalidating all input between ');
           write(Fe^,  '(',TokenPtr^.LinePos,':',TokenPtr^.CharPos,') and ');
           inc(TokenPtr);
           write(Fe^,  '(',TokenPtr^.LinePos,':',TokenPtr^.CharPos,')');
           writeln(Fe^, ' into a single <error>');
         end;
    Err_Halted:
         begin
           write(Fe^,'--> Parsing prematurely halted due to unsolveable error at');
           writeln(Fe^,' (',Token.LinePos:1,':',Token.CharPos:1,')');
         end;
    Err_Lexeme:
         begin
           write(Fe^,'Pos(',Token.LinePos:1,':',Token.CharPos:1,') ');
           write(Fe^,'Lexical Error: character "',CurChar,'" is skipped');
           writeln(Fe^);
         end;
    else begin
           write(Fe^,'Pos(',Token.LinePos:1,':',Token.CharPos:1,') ');
           write(Fe^,'Syntax Error(',ErrorNr,'): "',LexemeStr,'" found');
           if Token.Tag<>TokenPtr^.Tag then
           write(Fe^,' but <',TokenPtr^.Tag:1,'> expected');
           writeln(Fe^);
         end;
  end;
end;


{-----------------------------------------------------------------------------}
constructor TParser.Init(InFile:String);
{-----------------------------------------------------------------------------}
begin
  {$IFDEF Debug_HEAPMEM} ShowMem('MemUsage, at Parse start '); {$ENDIF}
  inherited init(InFile);
end;


{-----------------------------------------------------------------------------}
destructor TParser.Done;
{-----------------------------------------------------------------------------}
begin
  inherited done;
  {$IFDEF Debug_HEAPMEM} ShowMem('MemUsage, at Parse Done '); {$ENDIF}
end;


{-----------------------------------------------------------------------------}
Procedure TParser.DoAccept(TokenPtr:PToken);
{-----------------------------------------------------------------------------}
begin
  //replace this with you own code, to process for instance a parse tree tha has
  //been build during the parsing process
  writeln('input has been parsed succesfuly');
end;


{-----------------------------------------------------------------------------}
Procedure TParser.DoCleanup(TokenPtr:PToken);
{-----------------------------------------------------------------------------}
//replace this with you own code, to free memory from the heap (when applicable)
begin
end;  





{-----------------------------------------------------------------------------}
Procedure TParser.DoAction(T:PTokenList; RuleNr,TokenCnt:Integer);
{-----------------------------------------------------------------------------}
// Then call associated action routine, with a pointer to the first 
// left-hand item of the involved rule.
// When finished, replace first left hand token tag with a new non-terminal,
// and return the number of left-hand items
{-----------------------------------------------------------------------------}
##RULE-TABLE
Type  TParseEdge= procedure(T:PTokenList; S:PParser);
const ActionTable: array [0..%Count%] of Pointer  = (
##RULES
    {%Value.Padded%} (@TParser.Action_%ID%),	
##END-RULES
    {%Count%} (Nil)
  );
##END-RULE-TABLE
begin
  //This template is auto-generated by GoldParser
  //Your tokens of this rule are in T^[1], T^[2], T^[3] .... T^[Cnt]
  //replace it with you own code, to handle adequate semantic actions
  {$IFNDEF Debug_PARSER} if TokenCnt>0 then {$ENDIF}
  TParseEdge(ActionTable[RuleNr])(T, @Self);
end {DoAction}; 



##RULE-TABLE
##RULES
{-----------------------------------------------------------------------------}
PROCEDURE TParser.Action_%ID%(T:PTokenList); 
{-----------------------------------------------------------------------------}
// RULE(%Index%) =   %Description%
// COUNT:  %SymbolCount%     ^---(Tag=%NonterminalIndex%)
begin
  //This template is auto-generated by GoldParser
  //Your tokens of this rule are in T^[1], T^[2], T^[3] .... T^[Cnt]
  //replace it with you own code, to handle adequate semantic actions
  {$IFDEF Debug_ACTIONS} WriteLn(Fo^, 'Call Rule[%Index%]=%ID%(T,%SymbolCount%)'); {$ENDIF}
end {%ID%};


##END-RULES
##END-RULE-TABLE



##DELIMITER ','
##CHAR-SET-TABLE
{-----------------------------------------------------------------------------}
function TLexer.GetLexeme(SkipMode:Boolean):Integer;
{-----------------------------------------------------------------------------}
TYPE TLexEdge = record SetNr:Byte; L:Integer; end;
     PLexEdge = ^TLexEdge;
     TByteSet = set of Byte;  //32bytes long
     PByteSet = ^TByteSet;
CONST
  Empty=%Count%;
  CharSet : array [0..%Count%] of TByteSet = (
##DELIMITER ','  
##RANGE-CHARS '..'
##CHAR-SETS  
    {#%Index%} [%Chars.RangeList%],
##END-CHAR-SETS
    []
  );
##CHAR-SETS
  // #%Index% = '%Chars.XML%'
##END-CHAR-SETS
  // #%Count% =  <AnySet>
##END-CHAR-SET-TABLE
##DFA-TABLE
##DFA-STATES
  LexState%Index%: array [0..%EdgeCount%] of TLexEdge = ( 
##DFA-EDGES
    (SetNr:%CharSetIndex%;        L:%Target%), //%Chars%
##END-DFA-EDGES
    (SetNr:Empty; L:%AcceptIndex%)); //Tag:=%AcceptIndex%
##END-DFA-STATES
  LexStateList: array [0..%Count%] of PLexEdge = (
##DFA-STATES
    @LexState%Index%,
##END-DFA-STATES
    Nil
  );
  InitialState=%InitialState%;
##END-DFA-TABLE
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
// Reads a "lexeme", being a logical cohesive sequence of characters 
//
// if SkipMode is true, the reader becomes verry tolerant to errors
// It will consume all characters unless a lexeme of one or two
// characters is found. This enables you to skip block comments until a closing
// lexeme is found, like '}' or '*)'
//
// This lexeme reader can read one character ahead. The result is that it will read
// only a character, if the ahead character is not in conflict with a valid lexeme.
// This enables you to tokenize an input stream like('13..37') into a valid
// ('13'+'..'+'378') and not as ('13.') followed by an error.
// This mechanism uses the flag LexAccept, to delays acceptance of a char, until 
// its ahead char has proven not to be in conflict with a valid lexeme
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
label LexBegin, LexNextState, LexNextSet, LexDone;
var LexEdgePtr:PLexEdge;
    LexSetNr:Integer;
    LexChar:Char;
    LexAccept:Boolean; //reads one character ahead, to enable 123..456
    LexTag, State, OldState:Integer;
begin
  LexTag:=-1;
LexBegin:
  Token.LinePos:=LinePos;
  Token.CharPos:=CharPos;
  Token.Tag:=1; {Tag_ERROR};
  LexemeLen:=0;
  LexemeStr:='';
  State:=InitialState;
  OldState:=InitialState;
  LexChar:=CurChar;
  LexAccept:=False;
LexNextState:
  {$IFDEF Debug_LEXER}
  Write(Fo^,'  LexState=',State:3, ',  ord(Char)=',ord(LexChar):3,',  Char=');
  if ord(LexChar)>31 then write(Fo^,'"',LexChar,'"') else write(Fo^,'***');
  {$ENDIF Debug_LEXER}
  LexEdgePtr:=LexStateList[State];
  LexSetNr:=LexEdgePtr^.SetNr;
  {}
LexNextSet: {Loop}
  if (LexSetNr=Empty) then begin
    LexTag:=LexEdgePtr^.L;
    if (LexTag>=0) or SkipMode then GetChar else begin
      {$IFDEF Debug_LEXER} Write(Fo^,'   (not found, set back)'); {$ENDIF}
      //try to repair error by falling back to previous state;
      LexEdgePtr:=LexStateList[OldState];
      while (LexEdgePtr^.SetNr<>Empty) do inc(LexEdgePtr);
      LexTag:=LexEdgePtr^.L;
    end;
    goto LexDone;
  end;
  if (ord(LexChar) in CharSet[LexSetNr]) then begin
    {$IFDEF Debug_LEXER} WriteLn(Fo^,',  (Found in CharSet[',LexSetNr,'])'); {$ENDIF}
    OldState:=State;
    State:=LexEdgePtr^.L;
    if SkipMode and (LexemeLen=1) then goto LexBegin;
    if LexAccept then GetChar;
    LexChar:=NextChar; LexAccept:=True;
    goto LexNextState;
  end;
  inc(LexEdgePtr);
  LexSetNr:=LexEdgePtr^.SetNr;
  goto LexNextSet;  {next}
  {}
LexDone:
  if (LexTag<0) and (CurChar=EOF) then LexTag:=0; {TAG_EOF}
  {$IFDEF Debug_LEXER} WriteLn(Fo^,',  ==> Tag:=',LexTag); {$ENDIF}
  GetLexeme:=LexTag;
end;


{-----------------------------------------------------------------------------}
function TParser.DoParse(StackSize:Integer):Integer;
{-----------------------------------------------------------------------------}
##LALR-TABLE 
const Any  =TagCount;
      ActError=0; ActShift=1; ActReduce=2; ActNext=3; ActAccept=4;
      StartState=%InitialState%;       
type  TState =0..%Count%; 
      PState =^TState; 
      TParseEdge= record Tag:TTokenTag; Q:TState; Act:Byte; end;
      PParseEdge=^TParseEdge;
      TEntry = record EntryPtr:PParseEdge; EdgeCnt:Byte; end; 
var   StateStack, StateStackTop, StateStackEnd: PState;
      TokenStack, TokenStackTop:                PToken;
      ErrorState:    TState;
      ErrorStatePtr: PState;
      HasShifted:    Boolean;
##LALR-STATES 
const ParseState%Index%: array[0..%ActionCount%] of TParseEdge= (
##LALR-ACTIONS 
        (Tag:%SymbolIndex%; Q:%Value%; Act:%Action%), //%Action.Name%; 
##END-LALR-ACTIONS 
        (Tag:Any; Q:  0; Act:  0) );
##END-LALR-STATES 
const ParseStateList: array[TState] of TEntry= (
##LALR-STATES 
        (EntryPtr:@ParseState%Index%;	EdgeCnt:%ActionCount%),
##END-LALR-STATES 
        (EntryPtr:Nil; EdgeCnt:0 ));  
##END-LALR-TABLE 
##RULE-TABLE
const ReduceTable: array [0..%Count%] of record Cnt:Byte; Tag:TTokenTag; end = (
##RULES
    {%Value.Padded%} (Cnt:%SymbolCount%; Tag:%NonTerminalIndex%),	//%ID%,
##END-RULES
    {%Count%} (Cnt:0; Tag:ActError)
  );
##END-RULE-TABLE
  {}
  {}   
  {...........................................................................}
  procedure DebugStacks;
  {...........................................................................}
  var S:PState; T:PToken; N0,N1,k:Integer;
  begin
    S:=StateStack; N1:=1;
    while S<>StateStackTop do begin inc(S); inc(N1); end;
    if N1>12 then N0:=N1-12 else N0:=1;
    S:=StateStack; for k:=2 to N0 do inc(S);
    T:=TokenStack; for k:=2 to N0 do inc(T);
    write(Fo^,'  StateStack[',N0:1,'..',N1:1,']=[',S^:3);
    for k:=N0+1 to N1 do begin inc(S); write(Fo^,',',S^:3); end;
    writeln(Fo^,']');
    write(Fo^,'  TokenStack[',N0:1,'..',N1:1,']=[',T^.Tag:3);
    for k:=N0+1 to N1 do begin inc(T); write(Fo^,',',T^.Tag:3); end;
    writeln(Fo^,']');
  end;
  {} 
  {...........................................................................}
  function GetParseEdge(Tag:TTokenTag):PParseEdge;
  {...........................................................................}
  // returns the transition to a next state, or last tarnsition of an EdgeList
  var EdgePtr:PParseEdge; k:Integer;
  begin
    EdgePtr :=ParseStateList[StateStackTop^].EntryPtr;
    for k:=1 to ParseStateList[StateStackTop^].EdgeCnt do begin
      if (EdgePtr^.Tag=Tag) then break;
      inc(EdgePtr);
    end;  
    GetParseEdge:=EdgePtr;
  end;
  {}
  {...........................................................................}
  function GetExpect:TTokenTag;
  {...........................................................................}
  //inquire the parse tables, to identify what token was expected
  var EdgePtr:PParseEdge; Expect:TTokenTag; k,OK:Integer;
  begin
    OK:=0;
    EdgePtr :=ParseStateList[StateStackTop^].EntryPtr;
    for k:=1 to ParseStateList[StateStackTop^].EdgeCnt do begin
      if (EdgePtr^.Act=ActShift) and (EdgePtr^.Tag<>Tag_SynError) then begin
        inc(OK); Expect:=EdgePtr^.Tag;
      end;  
      inc(EdgePtr);
    end;
    if OK<>1 then Expect:=Token.Tag;
    GetExpect:=Expect;
  end;
  {}
  {...........................................................................}
  function HandleError:Boolean;
  {...........................................................................}
  var  EdgePtr:PParseEdge;  Invalidated:Integer;  NewError:Boolean;
  begin
    //[Step 1] check if previous repair has failed
    NewError:=(HasShifted or (ErrorStatePtr<>StateStackTop) or (ErrorState<>StateStackTop^));
    Invalidated  := 0;
    HasShifted   := False;
    ErrorStatePtr:= StateStackTop;
    ErrorState   := StateStackTop^;    
    //[Step 2] report the error, if it is a new one
    if NewError then begin
      inc(ErrorCnt);
      if (ErrorCnt<=ErrorMax) then begin
        TokenStackTop^.Tag:=GetExpect;
        DoError(-StateStackTop^,TokenStackTop);
        TokenStackTop^.Tag:=Token.Tag;
      end else begin
        DoError(Err_Overflow, TokenStackTop);
        HandleError:=False;
        Exit;
      end;
    end;
    //[Step 3] remove processed tokens, until an error-token is found with a shift action
    repeat
      if (TokenStackTop=TokenStack) then begin
         DoError(Err_Halted,TokenStackTop);
         exit; //error unsolveable due to lack of adequate error production
      end;
      if NewError or (Invalidated>1) then begin
        EdgePtr:=GetParseEdge(Tag_SynError);
        if  (EdgePtr^.Tag<>Any) and (EdgePtr^.Act=ActShift) then break;
      end;
      DoCleanup(TokenStackTop);
      dec(TokenStackTop);
      dec(StateStackTop);
      inc(Invalidated);
    until false;
    //[Step 4] push error on the stack, with position info on the error
    TokenStackTop^.Tag:=Tag_SynError;
    //[Step 5] skip tokens, and push the first valid one on the stack
    inc(StateStackTop); StateStackTop^:=EdgePtr^.Q;
    inc(TokenStackTop); TokenStackTop^:=Token;
    while GetParseEdge(Token.Tag)^.Tag=Any do begin
      if Token.Tag=Tag_EOF then break;
      GetToken; HasShifted:=True;
      TokenStackTop^:=Token;
      inc(Invalidated);
    end;
    {$IFDEF Debug_ERRORS} if (Invalidated>0) then DoError(Err_Skipped,TokenStackTop); {$ENDIF}
    HandleError:=(Token.Tag<>Tag_EOF);
  end;
  {}
label ParseNext, ParseDone, ParseOverflow;
var   Tag:TTokenTag;
      Cnt,RuleNr:Integer;
      EdgePtr:PParseEdge;
      TopToken:TToken;
begin
  GetToken; HasShifted:=True;
  ErrorState:=StartState;
  ErrorStatePtr:=Nil;
  GetMem(TokenStack, StackSize*sizeof(TokenStackTop^));
  GetMem(StateStack, StackSize*sizeof(StateStackTop^));
  TokenStackTop :=TokenStack;
  TokenStackTop^:=Token;
  StateStackTop :=StateStack;
  StateStackEnd :=StateStack; inc(StateStackEnd,StackSize-1);
  StateStackTop^:=StartState;
ParseNext: //Repeat
  if (StateStackTop=StateStackEnd) then goto ParseOverflow;
  {$IFDEF Debug_PARSER}
    DebugStacks;
    write (Fo^,'      (State=', StateStackTop^:2,', Tag=',TokenStackTop^.Tag:2,' ==> ');
  {$ENDIF}
  EdgePtr:=GetParseEdge(TokenStackTop^.Tag);
  case EdgePtr^.Act of
    ActError:
       begin {invalidate several tokens, in an attempt to recover from this error}
         {$IFDEF Debug_PARSER} writeln(Fo^,'Error)'); {$ENDIF}
         //first reduce to the highest rule, before handling the error
         EdgePtr:=GetParseEdge(Tag_SynError);
         if (EdgePtr^.Tag<>Any) and (EdgePtr^.Act<>1) then begin
           TokenStackTop^.Tag:=Tag_SynError;
           goto ParseNext;
         end;
         if HandleError then goto ParseNext else goto ParseDone;
       end;
    ActShift:
       begin {read and push another token on the stack, and analyze again }
         {$IFDEF Debug_PARSER} writeln(Fo^,'Q=', EdgePtr^.Q:2,', Shift)'); {$ENDIF}
         GetToken; HasShifted:=True;
         inc(StateStackTop); StateStackTop^:=EdgePtr^.Q;
         inc(TokenStackTop); TokenStackTop^:=Token;
         goto ParseNext;
       end;
    ActReduce: 
       begin {replace several tokens, by a single one, representing a rule}
         {$IFDEF Debug_PARSER} write (Fo^,'Q=', EdgePtr^.Q:2,', Reduce'); {$ENDIF}
         RuleNr:=EdgePtr^.Q;
         Cnt:=ReduceTable[RuleNr].Cnt;
         dec(TokenStackTop,Cnt);
         dec(StateStackTop,Cnt);
         {$IFDEF  Debug_PARSER} WriteLn(Fo^,'By ',Cnt,')'); {$ENDIF}
         DoAction(PTokenList(TokenStackTop), RuleNr, Cnt);
         TokenStackTop^.Tag:=ReduceTable[RuleNr].Tag;
         TokenStackTop^.Kind:=0; //flag this token as a "non-terminal" token
         goto ParseNext;
       end;
    ActNext: 
       begin {analyze again, but now from another state}
         {$IFDEF Debug_PARSER} writeln(Fo^,'Q=', EdgePtr^.Q:2,', Next)'); {$ENDIF}
         inc(StateStackTop); StateStackTop^:=EdgePtr^.Q;
         inc(TokenStackTop); TokenStackTop^:=Token;
         goto ParseNext;
       end;
    ActAccept:
         begin {startsymbol found, all parsing can be finished}
         {$IFDEF Debug_PARSER} writeln(Fo^,'Q=', EdgePtr^.Q:2,', Accept)'); {$ENDIF}
         dec(TokenStackTop);
         dec(StateStackTop);
         goto ParseDone;
       end;
  end {case};
ParseOverflow:
  DoError(Err_StackOverflow,Nil);
  inc(ErrorCnt);
ParseDone:
  while (TokenStackTop<>TokenStack) do begin //clean up
    DoCleanup(TokenStackTop);
    dec(TokenStackTop);
  end;
  TopToken:=TokenStackTop^;
  FreeMem(TokenStack, StackSize*sizeof(TokenStackTop^));
  FreeMem(StateStack, StackSize*sizeof(StateStackTop^));
  if (ErrorCnt=0) then DoAccept(@TopToken);
  {$IFDEF Debug_HEAPMEM} ShowMem('MemUsage, at Parse accept'); {$ENDIF}
  DoCleanup(@TopToken);
  {$IFDEF Debug_HEAPMEM} ShowMem('MemUsage, at Parse finish'); {$ENDIF}
  DoParse:=ErrorCnt;
end; {TParser.DoParse}




##PARAMETERS
var Parser:TParser;
    ErrorCnt:Integer;
begin
  if ParamCount=0 then begin
    Writeln(' Usage:   %OutputFileBase% Infile ');
    Writeln(' Usage:   %OutputFileBase% Infile OutFile');
    Writeln(' Usage:   %OutputFileBase% Infile OutFile ErrFile');
    exit;
  end else
  if ParamCount>=1 then begin
    Fo:=@OutPut; Fe:=@OutPut; Parser.Init(ParamStr(1)); 
    WriteLn('Parsing: "',ParamStr(1),'"');
  end;
  if ParamCount>=2 then begin
    new(Fo); assign(Fo^,ParamStr(2)); rewrite(Fo^);
  end;
  if ParamCount>=3 then begin
    New(Fe); assign(Fe^,ParamStr(3)); rewrite(Fe^);
  end;
  ErrorCnt:=Parser.DoParse(1000);
  Parser.Done;
  if ErrorCnt=0
  then write('done <SUCCES>, hit <return> :=')
  else write('done <FAILED> with ',ErrorCnt,' errors, hit <return> :=');
  readln;
  if ParamCount>=3 then begin close(Fe^); dispose(Fe); end;
  if ParamCount>=2 then begin close(Fo^); dispose(Fo); end;
end.
##END-PARAMETERS